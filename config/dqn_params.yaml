t3_lazy_couch_potato_v0: #namespace

  
  # dqn parameters - taking default from https://github.com/MichaelBosello/f1tenth-RL/blob/master/f1tenth-rl/rl_car_driver.py
  replay_buffer: 100000
  history_length: 2
  observation_steps: 500 # train only after this many steps (1 step = [history-length] frames) 
  target_model_update_freq: 500

  alpha: 0.00042
  gamma: 0.98
  epsilon: 1  # start with full exploration
  epsilon_min: 0.1
  epsilon_discount: 0.99994
  batch_size: 32  # batch size to be used in gradient descent
  
  # train params
  prioritized-replay: 'store_true'
  compress-replay: 'store_true'
  save-model-freq: 3000
  max-step-limit: 2000    # maximum steps that can be done in one episode
  eval-epoch-steps: 500   # how many steps (1 step = [history-length] frames) to run during an eval epoch
  train-epoch-steps: 2500 # how many steps (1 step = [history-length] frames) to run during a training epoch
  history_length: 2

  init_pos: 0.0 # Position in which the base will start 
  wait_time: 0.1 # Time to wait in the reset phases

  dqn_directory: '/home/user/catkin_ws/src/t3_lazy_couch_potato/training_results_dqn'